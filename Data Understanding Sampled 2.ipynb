{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c7f4f118-00e7-436e-8be4-9371d8fa08ce",
   "metadata": {},
   "source": [
    "# LIBRARIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "53cd40da-cdc9-4c58-b68f-a5c24d1f6e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import bigquery\n",
    "from google.oauth2 import service_account\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "from datetime import datetime\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad36747e-810f-49cc-98aa-2c06ff9187b1",
   "metadata": {},
   "source": [
    "# 1. Initial Setup and Connection to BigQuery\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "47c3a804-47e7-405a-bafa-9b5862dbd1f9",
   "metadata": {},
   "outputs": [
    {
     "ename": "RefreshError",
     "evalue": "('invalid_grant: Invalid JWT Signature.', {'error': 'invalid_grant', 'error_description': 'Invalid JWT Signature.'})",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRefreshError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 14\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m## Your logics implementation goes below\u001b[39;00m\n\u001b[0;32m     10\u001b[0m QUERY \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSELECT prod_id, prod_desc FROM `msba-emory.isom676_machine_learning.products` \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mWHERE prod_id = 20971208 \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLIMIT 5\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 14\u001b[0m query_job \u001b[38;5;241m=\u001b[39m client\u001b[38;5;241m.\u001b[39mquery(QUERY)  \u001b[38;5;66;03m# API request\u001b[39;00m\n\u001b[0;32m     15\u001b[0m rows \u001b[38;5;241m=\u001b[39m query_job\u001b[38;5;241m.\u001b[39mresult()  \u001b[38;5;66;03m# Waits for query to finish\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m row \u001b[38;5;129;01min\u001b[39;00m rows:\n",
      "File \u001b[1;32m~\\anaconda4\\Lib\\site-packages\\google\\cloud\\bigquery\\client.py:3492\u001b[0m, in \u001b[0;36mClient.query\u001b[1;34m(self, query, job_config, job_id, job_id_prefix, location, project, retry, timeout, job_retry, api_method)\u001b[0m\n\u001b[0;32m   3481\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _job_helpers\u001b[38;5;241m.\u001b[39mquery_jobs_query(\n\u001b[0;32m   3482\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   3483\u001b[0m         query,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3489\u001b[0m         job_retry,\n\u001b[0;32m   3490\u001b[0m     )\n\u001b[0;32m   3491\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m api_method \u001b[38;5;241m==\u001b[39m enums\u001b[38;5;241m.\u001b[39mQueryApiMethod\u001b[38;5;241m.\u001b[39mINSERT:\n\u001b[1;32m-> 3492\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _job_helpers\u001b[38;5;241m.\u001b[39mquery_jobs_insert(\n\u001b[0;32m   3493\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   3494\u001b[0m         query,\n\u001b[0;32m   3495\u001b[0m         job_config,\n\u001b[0;32m   3496\u001b[0m         job_id,\n\u001b[0;32m   3497\u001b[0m         job_id_prefix,\n\u001b[0;32m   3498\u001b[0m         location,\n\u001b[0;32m   3499\u001b[0m         project,\n\u001b[0;32m   3500\u001b[0m         retry,\n\u001b[0;32m   3501\u001b[0m         timeout,\n\u001b[0;32m   3502\u001b[0m         job_retry,\n\u001b[0;32m   3503\u001b[0m     )\n\u001b[0;32m   3504\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   3505\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGot unexpected value for api_method: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mrepr\u001b[39m(api_method)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda4\\Lib\\site-packages\\google\\cloud\\bigquery\\_job_helpers.py:159\u001b[0m, in \u001b[0;36mquery_jobs_insert\u001b[1;34m(client, query, job_config, job_id, job_id_prefix, location, project, retry, timeout, job_retry)\u001b[0m\n\u001b[0;32m    156\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    157\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m query_job\n\u001b[1;32m--> 159\u001b[0m future \u001b[38;5;241m=\u001b[39m do_query()\n\u001b[0;32m    160\u001b[0m \u001b[38;5;66;03m# The future might be in a failed state now, but if it's\u001b[39;00m\n\u001b[0;32m    161\u001b[0m \u001b[38;5;66;03m# unrecoverable, we'll find out when we ask for it's result, at which\u001b[39;00m\n\u001b[0;32m    162\u001b[0m \u001b[38;5;66;03m# point, we may retry.\u001b[39;00m\n\u001b[0;32m    163\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m job_id_given:\n",
      "File \u001b[1;32m~\\anaconda4\\Lib\\site-packages\\google\\cloud\\bigquery\\_job_helpers.py:136\u001b[0m, in \u001b[0;36mquery_jobs_insert.<locals>.do_query\u001b[1;34m()\u001b[0m\n\u001b[0;32m    133\u001b[0m query_job \u001b[38;5;241m=\u001b[39m job\u001b[38;5;241m.\u001b[39mQueryJob(job_ref, query, client\u001b[38;5;241m=\u001b[39mclient, job_config\u001b[38;5;241m=\u001b[39mjob_config)\n\u001b[0;32m    135\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 136\u001b[0m     query_job\u001b[38;5;241m.\u001b[39m_begin(retry\u001b[38;5;241m=\u001b[39mretry, timeout\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[0;32m    137\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core_exceptions\u001b[38;5;241m.\u001b[39mConflict \u001b[38;5;28;01mas\u001b[39;00m create_exc:\n\u001b[0;32m    138\u001b[0m     \u001b[38;5;66;03m# The thought is if someone is providing their own job IDs and they get\u001b[39;00m\n\u001b[0;32m    139\u001b[0m     \u001b[38;5;66;03m# their job ID generation wrong, this could end up returning results for\u001b[39;00m\n\u001b[0;32m    140\u001b[0m     \u001b[38;5;66;03m# the wrong query. We thus only try to recover if job ID was not given.\u001b[39;00m\n\u001b[0;32m    141\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m job_id_given:\n",
      "File \u001b[1;32m~\\anaconda4\\Lib\\site-packages\\google\\cloud\\bigquery\\job\\query.py:1383\u001b[0m, in \u001b[0;36mQueryJob._begin\u001b[1;34m(self, client, retry, timeout)\u001b[0m\n\u001b[0;32m   1363\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"API call:  begin the job via a POST request\u001b[39;00m\n\u001b[0;32m   1364\u001b[0m \n\u001b[0;32m   1365\u001b[0m \u001b[38;5;124;03mSee\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1379\u001b[0m \u001b[38;5;124;03m    ValueError: If the job has already begun.\u001b[39;00m\n\u001b[0;32m   1380\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1382\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1383\u001b[0m     \u001b[38;5;28msuper\u001b[39m(QueryJob, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m_begin(client\u001b[38;5;241m=\u001b[39mclient, retry\u001b[38;5;241m=\u001b[39mretry, timeout\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[0;32m   1384\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mGoogleAPICallError \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[0;32m   1385\u001b[0m     exc\u001b[38;5;241m.\u001b[39mmessage \u001b[38;5;241m=\u001b[39m _EXCEPTION_FOOTER_TEMPLATE\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m   1386\u001b[0m         message\u001b[38;5;241m=\u001b[39mexc\u001b[38;5;241m.\u001b[39mmessage, location\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlocation, job_id\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjob_id\n\u001b[0;32m   1387\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda4\\Lib\\site-packages\\google\\cloud\\bigquery\\job\\base.py:746\u001b[0m, in \u001b[0;36m_AsyncJob._begin\u001b[1;34m(self, client, retry, timeout)\u001b[0m\n\u001b[0;32m    743\u001b[0m \u001b[38;5;66;03m# jobs.insert is idempotent because we ensure that every new\u001b[39;00m\n\u001b[0;32m    744\u001b[0m \u001b[38;5;66;03m# job has an ID.\u001b[39;00m\n\u001b[0;32m    745\u001b[0m span_attributes \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpath\u001b[39m\u001b[38;5;124m\"\u001b[39m: path}\n\u001b[1;32m--> 746\u001b[0m api_response \u001b[38;5;241m=\u001b[39m client\u001b[38;5;241m.\u001b[39m_call_api(\n\u001b[0;32m    747\u001b[0m     retry,\n\u001b[0;32m    748\u001b[0m     span_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBigQuery.job.begin\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    749\u001b[0m     span_attributes\u001b[38;5;241m=\u001b[39mspan_attributes,\n\u001b[0;32m    750\u001b[0m     job_ref\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    751\u001b[0m     method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPOST\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    752\u001b[0m     path\u001b[38;5;241m=\u001b[39mpath,\n\u001b[0;32m    753\u001b[0m     data\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mto_api_repr(),\n\u001b[0;32m    754\u001b[0m     timeout\u001b[38;5;241m=\u001b[39mtimeout,\n\u001b[0;32m    755\u001b[0m )\n\u001b[0;32m    756\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_properties(api_response)\n",
      "File \u001b[1;32m~\\anaconda4\\Lib\\site-packages\\google\\cloud\\bigquery\\client.py:833\u001b[0m, in \u001b[0;36mClient._call_api\u001b[1;34m(self, retry, span_name, span_attributes, job_ref, headers, **kwargs)\u001b[0m\n\u001b[0;32m    829\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m span_name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    830\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m create_span(\n\u001b[0;32m    831\u001b[0m         name\u001b[38;5;241m=\u001b[39mspan_name, attributes\u001b[38;5;241m=\u001b[39mspan_attributes, client\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m, job_ref\u001b[38;5;241m=\u001b[39mjob_ref\n\u001b[0;32m    832\u001b[0m     ):\n\u001b[1;32m--> 833\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m call()\n\u001b[0;32m    835\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m call()\n",
      "File \u001b[1;32m~\\anaconda4\\Lib\\site-packages\\google\\api_core\\retry\\retry_unary.py:293\u001b[0m, in \u001b[0;36mRetry.__call__.<locals>.retry_wrapped_func\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    289\u001b[0m target \u001b[38;5;241m=\u001b[39m functools\u001b[38;5;241m.\u001b[39mpartial(func, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    290\u001b[0m sleep_generator \u001b[38;5;241m=\u001b[39m exponential_sleep_generator(\n\u001b[0;32m    291\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_initial, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maximum, multiplier\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_multiplier\n\u001b[0;32m    292\u001b[0m )\n\u001b[1;32m--> 293\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m retry_target(\n\u001b[0;32m    294\u001b[0m     target,\n\u001b[0;32m    295\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_predicate,\n\u001b[0;32m    296\u001b[0m     sleep_generator,\n\u001b[0;32m    297\u001b[0m     timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout,\n\u001b[0;32m    298\u001b[0m     on_error\u001b[38;5;241m=\u001b[39mon_error,\n\u001b[0;32m    299\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda4\\Lib\\site-packages\\google\\api_core\\retry\\retry_unary.py:153\u001b[0m, in \u001b[0;36mretry_target\u001b[1;34m(target, predicate, sleep_generator, timeout, on_error, exception_factory, **kwargs)\u001b[0m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[0;32m    150\u001b[0m \u001b[38;5;66;03m# This function explicitly must deal with broad exceptions.\u001b[39;00m\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[0;32m    152\u001b[0m     \u001b[38;5;66;03m# defer to shared logic for handling errors\u001b[39;00m\n\u001b[1;32m--> 153\u001b[0m     _retry_error_helper(\n\u001b[0;32m    154\u001b[0m         exc,\n\u001b[0;32m    155\u001b[0m         deadline,\n\u001b[0;32m    156\u001b[0m         sleep,\n\u001b[0;32m    157\u001b[0m         error_list,\n\u001b[0;32m    158\u001b[0m         predicate,\n\u001b[0;32m    159\u001b[0m         on_error,\n\u001b[0;32m    160\u001b[0m         exception_factory,\n\u001b[0;32m    161\u001b[0m         timeout,\n\u001b[0;32m    162\u001b[0m     )\n\u001b[0;32m    163\u001b[0m     \u001b[38;5;66;03m# if exception not raised, sleep before next attempt\u001b[39;00m\n\u001b[0;32m    164\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(sleep)\n",
      "File \u001b[1;32m~\\anaconda4\\Lib\\site-packages\\google\\api_core\\retry\\retry_base.py:212\u001b[0m, in \u001b[0;36m_retry_error_helper\u001b[1;34m(exc, deadline, next_sleep, error_list, predicate_fn, on_error_fn, exc_factory_fn, original_timeout)\u001b[0m\n\u001b[0;32m    206\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m predicate_fn(exc):\n\u001b[0;32m    207\u001b[0m     final_exc, source_exc \u001b[38;5;241m=\u001b[39m exc_factory_fn(\n\u001b[0;32m    208\u001b[0m         error_list,\n\u001b[0;32m    209\u001b[0m         RetryFailureReason\u001b[38;5;241m.\u001b[39mNON_RETRYABLE_ERROR,\n\u001b[0;32m    210\u001b[0m         original_timeout,\n\u001b[0;32m    211\u001b[0m     )\n\u001b[1;32m--> 212\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m final_exc \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msource_exc\u001b[39;00m\n\u001b[0;32m    213\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m on_error_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    214\u001b[0m     on_error_fn(exc)\n",
      "File \u001b[1;32m~\\anaconda4\\Lib\\site-packages\\google\\api_core\\retry\\retry_unary.py:144\u001b[0m, in \u001b[0;36mretry_target\u001b[1;34m(target, predicate, sleep_generator, timeout, on_error, exception_factory, **kwargs)\u001b[0m\n\u001b[0;32m    142\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sleep \u001b[38;5;129;01min\u001b[39;00m sleep_generator:\n\u001b[0;32m    143\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 144\u001b[0m         result \u001b[38;5;241m=\u001b[39m target()\n\u001b[0;32m    145\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39misawaitable(result):\n\u001b[0;32m    146\u001b[0m             warnings\u001b[38;5;241m.\u001b[39mwarn(_ASYNC_RETRY_WARNING)\n",
      "File \u001b[1;32m~\\anaconda4\\Lib\\site-packages\\google\\cloud\\_http\\__init__.py:482\u001b[0m, in \u001b[0;36mJSONConnection.api_request\u001b[1;34m(self, method, path, query_params, data, content_type, headers, api_base_url, api_version, expect_json, _target_object, timeout, extra_api_info)\u001b[0m\n\u001b[0;32m    479\u001b[0m     data \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mdumps(data)\n\u001b[0;32m    480\u001b[0m     content_type \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mapplication/json\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 482\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_request(\n\u001b[0;32m    483\u001b[0m     method\u001b[38;5;241m=\u001b[39mmethod,\n\u001b[0;32m    484\u001b[0m     url\u001b[38;5;241m=\u001b[39murl,\n\u001b[0;32m    485\u001b[0m     data\u001b[38;5;241m=\u001b[39mdata,\n\u001b[0;32m    486\u001b[0m     content_type\u001b[38;5;241m=\u001b[39mcontent_type,\n\u001b[0;32m    487\u001b[0m     headers\u001b[38;5;241m=\u001b[39mheaders,\n\u001b[0;32m    488\u001b[0m     target_object\u001b[38;5;241m=\u001b[39m_target_object,\n\u001b[0;32m    489\u001b[0m     timeout\u001b[38;5;241m=\u001b[39mtimeout,\n\u001b[0;32m    490\u001b[0m     extra_api_info\u001b[38;5;241m=\u001b[39mextra_api_info,\n\u001b[0;32m    491\u001b[0m )\n\u001b[0;32m    493\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;241m200\u001b[39m \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m300\u001b[39m:\n\u001b[0;32m    494\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mfrom_http_response(response)\n",
      "File \u001b[1;32m~\\anaconda4\\Lib\\site-packages\\google\\cloud\\_http\\__init__.py:341\u001b[0m, in \u001b[0;36mJSONConnection._make_request\u001b[1;34m(self, method, url, data, content_type, headers, target_object, timeout, extra_api_info)\u001b[0m\n\u001b[0;32m    338\u001b[0m     headers[CLIENT_INFO_HEADER] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muser_agent\n\u001b[0;32m    339\u001b[0m headers[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUser-Agent\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muser_agent\n\u001b[1;32m--> 341\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_do_request(\n\u001b[0;32m    342\u001b[0m     method, url, headers, data, target_object, timeout\u001b[38;5;241m=\u001b[39mtimeout\n\u001b[0;32m    343\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda4\\Lib\\site-packages\\google\\cloud\\_http\\__init__.py:379\u001b[0m, in \u001b[0;36mJSONConnection._do_request\u001b[1;34m(self, method, url, headers, data, target_object, timeout)\u001b[0m\n\u001b[0;32m    345\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_do_request\u001b[39m(\n\u001b[0;32m    346\u001b[0m     \u001b[38;5;28mself\u001b[39m, method, url, headers, data, target_object, timeout\u001b[38;5;241m=\u001b[39m_DEFAULT_TIMEOUT\n\u001b[0;32m    347\u001b[0m ):  \u001b[38;5;66;03m# pylint: disable=unused-argument\u001b[39;00m\n\u001b[0;32m    348\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Low-level helper:  perform the actual API request over HTTP.\u001b[39;00m\n\u001b[0;32m    349\u001b[0m \n\u001b[0;32m    350\u001b[0m \u001b[38;5;124;03m    Allows batch context managers to override and defer a request.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    377\u001b[0m \u001b[38;5;124;03m    :returns: The HTTP response.\u001b[39;00m\n\u001b[0;32m    378\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 379\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhttp\u001b[38;5;241m.\u001b[39mrequest(\n\u001b[0;32m    380\u001b[0m         url\u001b[38;5;241m=\u001b[39murl, method\u001b[38;5;241m=\u001b[39mmethod, headers\u001b[38;5;241m=\u001b[39mheaders, data\u001b[38;5;241m=\u001b[39mdata, timeout\u001b[38;5;241m=\u001b[39mtimeout\n\u001b[0;32m    381\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda4\\Lib\\site-packages\\google\\auth\\transport\\requests.py:534\u001b[0m, in \u001b[0;36mAuthorizedSession.request\u001b[1;34m(self, method, url, data, headers, max_allowed_time, timeout, **kwargs)\u001b[0m\n\u001b[0;32m    531\u001b[0m remaining_time \u001b[38;5;241m=\u001b[39m max_allowed_time\n\u001b[0;32m    533\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m TimeoutGuard(remaining_time) \u001b[38;5;28;01mas\u001b[39;00m guard:\n\u001b[1;32m--> 534\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcredentials\u001b[38;5;241m.\u001b[39mbefore_request(auth_request, method, url, request_headers)\n\u001b[0;32m    535\u001b[0m remaining_time \u001b[38;5;241m=\u001b[39m guard\u001b[38;5;241m.\u001b[39mremaining_timeout\n\u001b[0;32m    537\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m TimeoutGuard(remaining_time) \u001b[38;5;28;01mas\u001b[39;00m guard:\n",
      "File \u001b[1;32m~\\anaconda4\\Lib\\site-packages\\google\\auth\\credentials.py:239\u001b[0m, in \u001b[0;36mCredentials.before_request\u001b[1;34m(self, request, method, url, headers)\u001b[0m\n\u001b[0;32m    237\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_non_blocking_refresh(request)\n\u001b[0;32m    238\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 239\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_blocking_refresh(request)\n\u001b[0;32m    241\u001b[0m metrics\u001b[38;5;241m.\u001b[39madd_metric_header(headers, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_metric_header_for_usage())\n\u001b[0;32m    242\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply(headers)\n",
      "File \u001b[1;32m~\\anaconda4\\Lib\\site-packages\\google\\auth\\credentials.py:202\u001b[0m, in \u001b[0;36mCredentials._blocking_refresh\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m    200\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_blocking_refresh\u001b[39m(\u001b[38;5;28mself\u001b[39m, request):\n\u001b[0;32m    201\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalid:\n\u001b[1;32m--> 202\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrefresh(request)\n",
      "File \u001b[1;32m~\\anaconda4\\Lib\\site-packages\\google\\oauth2\\service_account.py:448\u001b[0m, in \u001b[0;36mCredentials.refresh\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m    446\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    447\u001b[0m     assertion \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_authorization_grant_assertion()\n\u001b[1;32m--> 448\u001b[0m     access_token, expiry, _ \u001b[38;5;241m=\u001b[39m _client\u001b[38;5;241m.\u001b[39mjwt_grant(\n\u001b[0;32m    449\u001b[0m         request, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_token_uri, assertion\n\u001b[0;32m    450\u001b[0m     )\n\u001b[0;32m    451\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtoken \u001b[38;5;241m=\u001b[39m access_token\n\u001b[0;32m    452\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexpiry \u001b[38;5;241m=\u001b[39m expiry\n",
      "File \u001b[1;32m~\\anaconda4\\Lib\\site-packages\\google\\oauth2\\_client.py:298\u001b[0m, in \u001b[0;36mjwt_grant\u001b[1;34m(request, token_uri, assertion, can_retry)\u001b[0m\n\u001b[0;32m    274\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Implements the JWT Profile for OAuth 2.0 Authorization Grants.\u001b[39;00m\n\u001b[0;32m    275\u001b[0m \n\u001b[0;32m    276\u001b[0m \u001b[38;5;124;03mFor more details, see `rfc7523 section 4`_.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    294\u001b[0m \u001b[38;5;124;03m.. _rfc7523 section 4: https://tools.ietf.org/html/rfc7523#section-4\u001b[39;00m\n\u001b[0;32m    295\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    296\u001b[0m body \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124massertion\u001b[39m\u001b[38;5;124m\"\u001b[39m: assertion, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgrant_type\u001b[39m\u001b[38;5;124m\"\u001b[39m: _JWT_GRANT_TYPE}\n\u001b[1;32m--> 298\u001b[0m response_data \u001b[38;5;241m=\u001b[39m _token_endpoint_request(\n\u001b[0;32m    299\u001b[0m     request,\n\u001b[0;32m    300\u001b[0m     token_uri,\n\u001b[0;32m    301\u001b[0m     body,\n\u001b[0;32m    302\u001b[0m     can_retry\u001b[38;5;241m=\u001b[39mcan_retry,\n\u001b[0;32m    303\u001b[0m     headers\u001b[38;5;241m=\u001b[39m{\n\u001b[0;32m    304\u001b[0m         metrics\u001b[38;5;241m.\u001b[39mAPI_CLIENT_HEADER: metrics\u001b[38;5;241m.\u001b[39mtoken_request_access_token_sa_assertion()\n\u001b[0;32m    305\u001b[0m     },\n\u001b[0;32m    306\u001b[0m )\n\u001b[0;32m    308\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    309\u001b[0m     access_token \u001b[38;5;241m=\u001b[39m response_data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maccess_token\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32m~\\anaconda4\\Lib\\site-packages\\google\\oauth2\\_client.py:269\u001b[0m, in \u001b[0;36m_token_endpoint_request\u001b[1;34m(request, token_uri, body, access_token, use_json, can_retry, headers, **kwargs)\u001b[0m\n\u001b[0;32m    258\u001b[0m response_status_ok, response_data, retryable_error \u001b[38;5;241m=\u001b[39m _token_endpoint_request_no_throw(\n\u001b[0;32m    259\u001b[0m     request,\n\u001b[0;32m    260\u001b[0m     token_uri,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    266\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    267\u001b[0m )\n\u001b[0;32m    268\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m response_status_ok:\n\u001b[1;32m--> 269\u001b[0m     _handle_error_response(response_data, retryable_error)\n\u001b[0;32m    270\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response_data\n",
      "File \u001b[1;32m~\\anaconda4\\Lib\\site-packages\\google\\oauth2\\_client.py:68\u001b[0m, in \u001b[0;36m_handle_error_response\u001b[1;34m(response_data, retryable_error)\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mKeyError\u001b[39;00m, \u001b[38;5;167;01mValueError\u001b[39;00m):\n\u001b[0;32m     66\u001b[0m     error_details \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mdumps(response_data)\n\u001b[1;32m---> 68\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mRefreshError(\n\u001b[0;32m     69\u001b[0m     error_details, response_data, retryable\u001b[38;5;241m=\u001b[39mretryable_error\n\u001b[0;32m     70\u001b[0m )\n",
      "\u001b[1;31mRefreshError\u001b[0m: ('invalid_grant: Invalid JWT Signature.', {'error': 'invalid_grant', 'error_description': 'Invalid JWT Signature.'})"
     ]
    }
   ],
   "source": [
    "\n",
    "## construct credentials from service account key file\n",
    "credentials = service_account.Credentials.from_service_account_file(\n",
    "    'C:\\\\Users\\\\ivani\\\\Documents\\\\Emory University\\\\Spring 2025\\\\Machine Learning At Scale\\\\iumuhoz_bq (1)\\\\iumuhoz-isom676-srvacct.json') ## relative file path\n",
    "## '/mnt/c/Users/PGU6/workspace-GBS/student-technology-tools/docs/gcp/.ssl/bq_srv_acct.json') ## absolute file path\n",
    "\n",
    "## construct a BigQuery client object\n",
    "client = bigquery.Client(credentials=credentials)\n",
    "\n",
    "## Your logics implementation goes below\n",
    "QUERY = (\n",
    "    'SELECT prod_id, prod_desc FROM `msba-emory.isom676_machine_learning.products` '\n",
    "    'WHERE prod_id = 20971208 '\n",
    "    'LIMIT 5')\n",
    "query_job = client.query(QUERY)  # API request\n",
    "rows = query_job.result()  # Waits for query to finish\n",
    "\n",
    "for row in rows:\n",
    "    print(row[0]) ## by index\n",
    "    print(row.prod_id) ## by column prod_id\n",
    "    print(row.prod_desc) ## by column prod_id\n",
    "\n",
    "print(\"Connect and query BigQuery successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e39c7bb-5a62-4f0b-8f7a-46c8839786ad",
   "metadata": {},
   "source": [
    "# 2. Stratified Sampling of Transactions Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7f51af2-b136-476b-9d21-05d5f9a9b943",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's first get the list of stores to use for stratification\n",
    "STORES_QUERY = \"\"\"\n",
    "SELECT DISTINCT store_id\n",
    "FROM `msba-emory.isom676_machine_learning.transactions`\n",
    "\"\"\"\n",
    "\n",
    "print(\"Retrieving list of stores...\")\n",
    "stores = client.query(STORES_QUERY).to_dataframe()\n",
    "store_ids = stores['store_id'].tolist()\n",
    "print(f\"Found {len(store_ids)} stores\")\n",
    "\n",
    "# sample a  small percentage from each store\n",
    "# 0.1% should be manageable while still giving enough data\n",
    "sample_fraction = 0.001  # 0.1%\n",
    "print(f\"Will sample {sample_fraction*100:.2f}% from each store\")\n",
    "\n",
    "# Create an empty dataframe to hold our sample\n",
    "transactions_sample = pd.DataFrame()\n",
    "\n",
    "# Sample from each store separately using a simpler approach\n",
    "for i, store_id in enumerate(store_ids):\n",
    "    print(f\"Sampling from store {store_id} ({i+1}/{len(store_ids)})...\")\n",
    "    \n",
    "    # First, count the number of transactions for this store\n",
    "    COUNT_QUERY = f\"\"\"\n",
    "    SELECT COUNT(*) as count\n",
    "    FROM `msba-emory.isom676_machine_learning.transactions`\n",
    "    WHERE store_id = {store_id}\n",
    "    \"\"\"\n",
    "    \n",
    "    count_result = client.query(COUNT_QUERY).to_dataframe()\n",
    "    store_count = count_result['count'].iloc[0]\n",
    "    \n",
    "    # Calculate sample size for this store\n",
    "    sample_size = max(1, int(store_count * sample_fraction))\n",
    "    \n",
    "    # Now sample from this store\n",
    "    STORE_SAMPLE_QUERY = f\"\"\"\n",
    "    SELECT \n",
    "        cust_id,\n",
    "        store_id,\n",
    "        prod_id,\n",
    "        trans_id,\n",
    "        trans_dt,\n",
    "        sales_qty,\n",
    "        sales_wgt,\n",
    "        sales_amt\n",
    "    FROM `msba-emory.isom676_machine_learning.transactions`\n",
    "    WHERE store_id = {store_id}\n",
    "    ORDER BY RAND()\n",
    "    LIMIT {sample_size}\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        store_sample = client.query(STORE_SAMPLE_QUERY).to_dataframe()\n",
    "        print(f\"  - Sampled {len(store_sample)} transactions from store {store_id}\")\n",
    "        \n",
    "        # Add to our combined sample\n",
    "        transactions_sample = pd.concat([transactions_sample, store_sample])\n",
    "    except Exception as e:\n",
    "        print(f\"  - Error sampling store {store_id}: {str(e)}\")\n",
    "\n",
    "print(f\"\\nFinished sampling. Total sampled transactions: {len(transactions_sample):,}\")\n",
    "\n",
    "# Convert date column to datetime\n",
    "transactions_sample['trans_dt'] = pd.to_datetime(transactions_sample['trans_dt'])\n",
    "\n",
    "# Save sampled transactions for later use\n",
    "transactions_sample.to_csv('transactions_sample.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "801fb1bf-798b-47dc-8ccb-cdc58cf130e5",
   "metadata": {},
   "source": [
    "# 3.  Retrieve Full Products Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1100cf95-f244-4913-b866-168e91c2107e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the complete products dataset (no sampling needed)\n",
    "PRODUCTS_QUERY = \"\"\"\n",
    "SELECT *\n",
    "FROM `msba-emory.isom676_machine_learning.products`\n",
    "\"\"\"\n",
    "\n",
    "print(\"\\nRetrieving full products dataset...\")\n",
    "products = client.query(PRODUCTS_QUERY).to_dataframe()\n",
    "print(f\"Retrieved {len(products):,} products\")\n",
    "\n",
    "# Save products data for later use\n",
    "products.to_csv('products.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "840e0626-410b-42be-ac49-b771632be1c2",
   "metadata": {},
   "source": [
    "# 4. Initial Data Exploration and Quality Assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dea4229-7588-403f-bc14-029d08df0742",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data from saved files (if already saved)\n",
    "try:\n",
    "    transactions_sample = pd.read_csv('transactions_sample.csv')\n",
    "    transactions_sample['trans_dt'] = pd.to_datetime(transactions_sample['trans_dt'])\n",
    "    products = pd.read_csv('products.csv')\n",
    "    print(\"Loaded data from saved CSV files.\")\n",
    "except:\n",
    "    print(\"Using data from previous steps.\")\n",
    "\n",
    "print(\"\\n=== Initial Data Exploration ===\")\n",
    "\n",
    "# Basic information about the datasets\n",
    "print(\"\\nTransactions Sample Overview:\")\n",
    "print(f\"• Number of transactions: {len(transactions_sample):,}\")\n",
    "print(f\"• Number of unique customers: {transactions_sample['cust_id'].nunique():,}\")\n",
    "print(f\"• Number of unique stores: {transactions_sample['store_id'].nunique():,}\")\n",
    "print(f\"• Number of unique products: {transactions_sample['prod_id'].nunique():,}\")\n",
    "print(f\"• Date range: {transactions_sample['trans_dt'].min().date()} to {transactions_sample['trans_dt'].max().date()}\")\n",
    "print(f\"• Total sales amount: ${transactions_sample['sales_amt'].sum():,.2f}\")\n",
    "\n",
    "print(\"\\nTransactions Data Types:\")\n",
    "print(transactions_sample.dtypes)\n",
    "\n",
    "print(\"\\nTransactions Summary Statistics:\")\n",
    "print(transactions_sample.describe().round(2))\n",
    "\n",
    "print(\"\\nProducts Dataset Overview:\")\n",
    "print(f\"• Number of products: {len(products):,}\")\n",
    "if 'prod_category' in products.columns:\n",
    "    print(f\"• Number of categories: {products['prod_category'].nunique():,}\")\n",
    "if 'prod_subcategory' in products.columns:\n",
    "    print(f\"• Number of subcategories: {products['prod_subcategory'].nunique():,}\")\n",
    "if 'prod_section' in products.columns:\n",
    "    print(f\"• Number of sections: {products['prod_section'].nunique():,}\")\n",
    "\n",
    "print(\"\\nProducts Data Types:\")\n",
    "print(products.dtypes)\n",
    "\n",
    "# Check for missing values in both datasets\n",
    "print(\"\\n=== Missing Values Analysis ===\")\n",
    "trans_missing = transactions_sample.isnull().sum()\n",
    "prod_missing = products.isnull().sum()\n",
    "\n",
    "print(\"\\nTransactions - Missing values:\")\n",
    "if trans_missing.sum() > 0:\n",
    "    for col, count in trans_missing[trans_missing > 0].items():\n",
    "        print(f\"• Column '{col}': {count:,} missing values ({count/len(transactions_sample)*100:.2f}%)\")\n",
    "else:\n",
    "    print(\"No missing values found in transactions data.\")\n",
    "\n",
    "print(\"\\nProducts - Missing values:\")\n",
    "if prod_missing.sum() > 0:\n",
    "    for col, count in prod_missing[prod_missing > 0].items():\n",
    "        print(f\"• Column '{col}': {count:,} missing values ({count/len(products)*100:.2f}%)\")\n",
    "else:\n",
    "    print(\"No missing values found in products data.\")\n",
    "\n",
    "# Store pre-cleaning metrics for later comparison\n",
    "original_missing_count = transactions_sample.isnull().sum().sum() + products.isnull().sum().sum()\n",
    "original_neg_sales_count = len(transactions_sample[transactions_sample['sales_amt'] < 0])\n",
    "original_zero_neg_qty_count = len(transactions_sample[transactions_sample['sales_qty'] <= 0])\n",
    "\n",
    "print(f\"\\nPre-cleaning metrics:\")\n",
    "print(f\"• Total missing values: {original_missing_count:,}\")\n",
    "print(f\"• Negative sales transactions: {original_neg_sales_count:,} ({original_neg_sales_count/len(transactions_sample)*100:.2f}%)\")\n",
    "print(f\"• Zero/negative quantity transactions: {original_zero_neg_qty_count:,} ({original_zero_neg_qty_count/len(transactions_sample)*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c4d377f-cfb8-403a-af8b-83580e4ee689",
   "metadata": {},
   "source": [
    "# 5. Pre-Cleaning Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be1fb5dc-d2d1-4f4b-93dd-1cb7897fc613",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n=== Creating Pre-Cleaning Visualizations ===\")\n",
    "\n",
    "# 1. Distribution of sales amounts\n",
    "plt.figure(figsize=(15, 7))\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.histplot(transactions_sample['sales_amt'].clip(upper=transactions_sample['sales_amt'].quantile(0.99)), \n",
    "             bins=50, kde=True, color='blue')\n",
    "plt.title('Sales Amount Distribution (99th Percentile)\\nBefore Cleaning', fontsize=14)\n",
    "plt.xlabel('Sales Amount ($)', fontsize=12)\n",
    "plt.ylabel('Frequency', fontsize=12)\n",
    "\n",
    "# Sales amount boxplot\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.boxplot(y=transactions_sample['sales_amt'].clip(upper=transactions_sample['sales_amt'].quantile(0.99)), color='blue')\n",
    "plt.title('Sales Amount Boxplot (99th Percentile)\\nBefore Cleaning', fontsize=14)\n",
    "plt.ylabel('Sales Amount ($)', fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.savefig('visualizations/before_cleaning_sales_distribution.png')\n",
    "plt.show()\n",
    "\n",
    "# 2. Distribution of sales quantities\n",
    "plt.figure(figsize=(15, 7))\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.histplot(transactions_sample['sales_qty'].clip(upper=transactions_sample['sales_qty'].quantile(0.99)), \n",
    "             bins=50, kde=True, color='green')\n",
    "plt.title('Sales Quantity Distribution (99th Percentile)\\nBefore Cleaning', fontsize=14)\n",
    "plt.xlabel('Sales Quantity', fontsize=12)\n",
    "plt.ylabel('Frequency', fontsize=12)\n",
    "\n",
    "# Sales quantity boxplot\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.boxplot(y=transactions_sample['sales_qty'].clip(upper=transactions_sample['sales_qty'].quantile(0.99)), color='green')\n",
    "plt.title('Sales Quantity Boxplot (99th Percentile)\\nBefore Cleaning', fontsize=14)\n",
    "plt.ylabel('Sales Quantity', fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.savefig('visualizations/before_cleaning_quantity_distribution.png')\n",
    "plt.show()\n",
    "\n",
    "# 3. Missing values heatmap\n",
    "plt.figure(figsize=(14, 6))\n",
    "transactions_missing = transactions_sample.isnull()\n",
    "products_missing = products.isnull()\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.heatmap(transactions_missing.sample(min(1000, len(transactions_missing))), \n",
    "            cbar=False, cmap='viridis', yticklabels=False)\n",
    "plt.title('Missing Values in Transactions (Sample)\\nBefore Cleaning', fontsize=14)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.heatmap(products_missing.sample(min(1000, len(products_missing))), \n",
    "            cbar=False, cmap='viridis', yticklabels=False)\n",
    "plt.title('Missing Values in Products (Sample)\\nBefore Cleaning', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.savefig('visualizations/before_cleaning_missing_values.png')\n",
    "plt.show()\n",
    "\n",
    "# 4. Negative sales amounts (if any)\n",
    "neg_sales = transactions_sample[transactions_sample['sales_amt'] < 0]\n",
    "if len(neg_sales) > 0:\n",
    "    plt.figure(figsize=(14, 6))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    sns.histplot(neg_sales['sales_amt'], bins=30, kde=True, color='red')\n",
    "    plt.title('Distribution of Negative Sales Amounts\\nBefore Cleaning', fontsize=14)\n",
    "    plt.xlabel('Sales Amount ($)', fontsize=12)\n",
    "    plt.ylabel('Frequency', fontsize=12)\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    monthly_returns = neg_sales.groupby(neg_sales['trans_dt'].dt.to_period('M'))['sales_amt'].sum()\n",
    "    monthly_returns = monthly_returns.reset_index()\n",
    "    monthly_returns['trans_dt'] = monthly_returns['trans_dt'].astype(str)\n",
    "    plt.bar(monthly_returns['trans_dt'], monthly_returns['sales_amt'].abs(), color='red')\n",
    "    plt.title('Monthly Return Amounts\\nBefore Cleaning', fontsize=14)\n",
    "    plt.xlabel('Month', fontsize=12)\n",
    "    plt.ylabel('Total Returns ($)', fontsize=12)\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('visualizations/before_cleaning_negative_sales.png')\n",
    "    plt.show()\n",
    "\n",
    "# 5. Zero or negative quantities (if any)\n",
    "zero_neg_qty = transactions_sample[transactions_sample['sales_qty'] <= 0]\n",
    "if len(zero_neg_qty) > 0:\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    # Create a count by value for the most common zero/negative quantities\n",
    "    qty_counts = zero_neg_qty['sales_qty'].value_counts().sort_index().head(10)\n",
    "    plt.bar(qty_counts.index.astype(str), qty_counts.values, color='purple')\n",
    "    plt.title('Count of Zero or Negative Quantities\\nBefore Cleaning', fontsize=14)\n",
    "    plt.xlabel('Sales Quantity', fontsize=12)\n",
    "    plt.ylabel('Count', fontsize=12)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('visualizations/before_cleaning_zero_negative_qty.png')\n",
    "    plt.show()\n",
    "\n",
    "# 6. Sales over time\n",
    "plt.figure(figsize=(14, 7))\n",
    "# Group by day and calculate total sales\n",
    "daily_sales = transactions_sample.groupby(transactions_sample['trans_dt'].dt.date)['sales_amt'].sum()\n",
    "plt.plot(daily_sales.index, daily_sales.values, color='blue', linewidth=2)\n",
    "plt.title('Daily Sales Over Time\\nBefore Cleaning', fontsize=14)\n",
    "plt.xlabel('Date', fontsize=12)\n",
    "plt.ylabel('Total Sales Amount ($)', fontsize=12)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig('visualizations/before_cleaning_daily_sales.png')\n",
    "plt.show()\n",
    "\n",
    "print(\"Pre-cleaning visualizations created and displayed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1557c9de-e171-469c-92f4-8831db4b0bb2",
   "metadata": {},
   "source": [
    "# 6. Data Cleaning and Anomaly Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1f3380b-860d-4bf5-ac7f-b940a889f9a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n=== Data Cleaning and Anomaly Detection ===\")\n",
    "\n",
    "# Create copies of the data for cleaning\n",
    "cleaned_trans = transactions_sample.copy()\n",
    "cleaned_prods = products.copy()\n",
    "\n",
    "# 1. Handle missing values\n",
    "print(\"\\n--- Handling Missing Values ---\")\n",
    "\n",
    "# For transactions\n",
    "if trans_missing.sum() > 0:\n",
    "    for col in trans_missing[trans_missing > 0].index:\n",
    "        if col == 'sales_qty':\n",
    "            cleaned_trans['sales_qty'] = cleaned_trans['sales_qty'].fillna(1)\n",
    "            print(f\"• Filled {trans_missing[col]:,} missing values in 'sales_qty' with default value 1\")\n",
    "            print(\"  Rationale: In retail, a missing quantity most commonly represents a single item purchase. Using 1 as default preserves transaction integrity.\")\n",
    "        elif col == 'sales_wgt':\n",
    "            cleaned_trans['sales_wgt'] = cleaned_trans['sales_wgt'].fillna(0)\n",
    "            print(f\"• Filled {trans_missing[col]:,} missing values in 'sales_wgt' with default value 0\")\n",
    "            print(\"  Rationale: Missing weight values likely indicate non-weight-based products. Zero is appropriate as these products aren't sold by weight.\")\n",
    "        elif col == 'sales_amt':\n",
    "            # For missing sales amounts, use the median price for that product\n",
    "            missing_amt_count = cleaned_trans['sales_amt'].isnull().sum()\n",
    "            for prod_id in cleaned_trans[cleaned_trans['sales_amt'].isnull()]['prod_id'].unique():\n",
    "                # Get median price per unit for this product\n",
    "                prod_trans = cleaned_trans[(cleaned_trans['prod_id'] == prod_id) & \n",
    "                                          cleaned_trans['sales_amt'].notnull() & \n",
    "                                          (cleaned_trans['sales_qty'] > 0)]\n",
    "                if len(prod_trans) > 0:\n",
    "                    median_price_per_unit = (prod_trans['sales_amt'] / prod_trans['sales_qty']).median()\n",
    "                    # Apply to missing values\n",
    "                    mask = (cleaned_trans['prod_id'] == prod_id) & cleaned_trans['sales_amt'].isnull()\n",
    "                    cleaned_trans.loc[mask, 'sales_amt'] = cleaned_trans.loc[mask, 'sales_qty'] * median_price_per_unit\n",
    "            \n",
    "            # For any remaining missing values, use overall median price\n",
    "            remaining_missing = cleaned_trans['sales_amt'].isnull().sum()\n",
    "            if remaining_missing > 0:\n",
    "                overall_median = cleaned_trans[cleaned_trans['sales_amt'].notnull()]['sales_amt'].median()\n",
    "                cleaned_trans['sales_amt'] = cleaned_trans['sales_amt'].fillna(overall_median)\n",
    "                print(f\"  Note: Used global median price for {remaining_missing:,} transactions where product-specific data was unavailable\")\n",
    "            \n",
    "            print(f\"• Filled {missing_amt_count:,} missing values in 'sales_amt' using product-specific median prices\")\n",
    "            print(\"  Method: Calculated median price per unit for each product, then multiplied by quantity\")\n",
    "            print(\"  Rationale: Product-specific pricing maintains data accuracy by respecting the unique price point of each product, rather than using a global value\")\n",
    "else:\n",
    "    print(\"No missing values in transactions data to handle.\")\n",
    "\n",
    "# For products\n",
    "if prod_missing.sum() > 0:\n",
    "    for col in prod_missing[prod_missing > 0].index:\n",
    "        if col == 'prod_desc':\n",
    "            cleaned_prods['prod_desc'] = cleaned_prods['prod_desc'].fillna(\"MISSING DESCRIPTION\")\n",
    "            print(f\"• Filled {prod_missing[col]:,} missing values in 'prod_desc' with 'MISSING DESCRIPTION'\")\n",
    "            print(\"  Rationale: Using a distinctive placeholder allows easy identification of these products for further investigation while maintaining data completeness\")\n",
    "        else:\n",
    "            cleaned_prods[col] = cleaned_prods[col].fillna(\"Unknown\")\n",
    "            print(f\"• Filled {prod_missing[col]:,} missing values in '{col}' with 'Unknown'\")\n",
    "            print(\"  Method: Category imputation with standard placeholder\")\n",
    "            print(\"  Rationale: Preserves these products in the dataset while clearly indicating missing information\")\n",
    "else:\n",
    "    print(\"No missing values in products data to handle.\")\n",
    "\n",
    "# 2. Text standardization\n",
    "print(\"\\n--- Standardizing Text Fields ---\")\n",
    "if 'prod_desc' in cleaned_prods.columns:\n",
    "    cleaned_prods['prod_desc'] = cleaned_prods['prod_desc'].astype(str).str.strip().str.lower()\n",
    "    print(\"• Standardized product descriptions (lowercase, trimmed spaces)\")\n",
    "    print(\"  Method: Applied string transformation functions (strip and lowercase)\")\n",
    "    print(\"  Rationale: Ensures consistent text formatting for accurate matching and analysis, eliminating variations due to capitalization and whitespace\")\n",
    "\n",
    "if 'prod_category' in cleaned_prods.columns:\n",
    "    cleaned_prods['prod_category'] = cleaned_prods['prod_category'].astype(str).str.strip()\n",
    "    print(\"• Standardized product categories (trimmed spaces)\")\n",
    "    print(\"  Rationale: Category names need consistent formatting for correct grouping in analyses\")\n",
    "\n",
    "if 'prod_subcategory' in cleaned_prods.columns:\n",
    "    cleaned_prods['prod_subcategory'] = cleaned_prods['prod_subcategory'].astype(str).str.strip()\n",
    "    print(\"• Standardized product subcategories (trimmed spaces)\")\n",
    "    print(\"  Rationale: Ensures consistent hierarchical categorization and prevents duplicate categories due to whitespace variations\")\n",
    "\n",
    "# 3. Handle anomalies\n",
    "print(\"\\n--- Detecting and Handling Anomalies ---\")\n",
    "\n",
    "# 3.1 Identify and flag returns (negative sales)\n",
    "neg_sales = cleaned_trans[cleaned_trans['sales_amt'] < 0]\n",
    "if len(neg_sales) > 0:\n",
    "    cleaned_trans['is_return'] = cleaned_trans['sales_amt'] < 0\n",
    "    print(f\"• Identified and flagged {len(neg_sales):,} transactions as returns (negative sales)\")\n",
    "    print(f\"  Average return amount: ${neg_sales['sales_amt'].mean():.2f}\")\n",
    "    print(\"  Method: Created boolean flag column 'is_return' instead of removing these records\")\n",
    "    print(\"  Rationale: Negative sales values likely represent valid returns or refunds - removing them would artificially inflate sales metrics, while flagging allows separate analysis of return patterns\")\n",
    "\n",
    "# 3.2 Fix zero or negative quantities with positive sales\n",
    "zero_with_sales = (cleaned_trans['sales_qty'] <= 0) & (cleaned_trans['sales_amt'] > 0)\n",
    "if zero_with_sales.sum() > 0:\n",
    "    cleaned_trans.loc[zero_with_sales, 'sales_qty'] = 1\n",
    "    print(f\"• Fixed {zero_with_sales.sum():,} transactions with zero/negative quantities but positive sales\")\n",
    "    print(\"  Method: Replaced illogical values with the minimum valid quantity (1)\")\n",
    "    print(\"  Rationale: These are likely data entry errors since a transaction cannot have a positive sales amount with zero or negative quantity. Setting to 1 preserves the transaction while resolving the logical inconsistency.\")\n",
    "\n",
    "# 3.3 Identify outlier customers\n",
    "# Calculate customer metrics\n",
    "customer_stats = cleaned_trans.groupby('cust_id').agg({\n",
    "    'sales_amt': 'sum',\n",
    "    'trans_id': 'nunique',\n",
    "    'prod_id': 'nunique'\n",
    "})\n",
    "customer_stats.columns = ['total_spent', 'num_transactions', 'unique_products']\n",
    "\n",
    "# Use Z-score method to identify outliers (3 standard deviations)\n",
    "outlier_customers = set()\n",
    "for col in ['total_spent', 'num_transactions', 'unique_products']:\n",
    "    mean = customer_stats[col].mean()\n",
    "    std = customer_stats[col].std()\n",
    "    threshold = mean + 3 * std\n",
    "    outliers = customer_stats[customer_stats[col] > threshold]\n",
    "    \n",
    "    if len(outliers) > 0:\n",
    "        print(f\"• Found {len(outliers):,} outlier customers based on {col}\")\n",
    "        print(f\"  Threshold: {threshold:.2f}, Max value: {outliers[col].max():.2f}\")\n",
    "        outlier_customers.update(outliers.index)\n",
    "\n",
    "# Flag outlier customers in the transactions\n",
    "cleaned_trans['is_outlier_customer'] = cleaned_trans['cust_id'].isin(outlier_customers)\n",
    "print(f\"• Flagged {len(outlier_customers):,} unique outlier customers in the dataset\")\n",
    "print(\"  Method: Z-score outlier detection with threshold of 3 standard deviations\")\n",
    "print(\"  Rationale: Using statistical thresholds identifies true outliers while preserving their data for analysis. These could represent high-value customers, business accounts, or potential data errors that warrant special attention.\")\n",
    "\n",
    "# 3.4 Identify stores with unusual transaction counts\n",
    "store_stats = cleaned_trans.groupby('store_id').agg({\n",
    "    'trans_id': 'nunique',\n",
    "    'cust_id': 'nunique'\n",
    "})\n",
    "store_stats.columns = ['num_transactions', 'num_customers']\n",
    "\n",
    "# Find stores with too few transactions (bottom 5%)\n",
    "low_threshold = np.percentile(store_stats['num_transactions'], 5)\n",
    "low_activity_stores = store_stats[store_stats['num_transactions'] < low_threshold]\n",
    "\n",
    "if len(low_activity_stores) > 0:\n",
    "    cleaned_trans['is_low_activity_store'] = cleaned_trans['store_id'].isin(low_activity_stores.index)\n",
    "    print(f\"• Identified {len(low_activity_stores):,} stores with unusually low transaction counts\")\n",
    "    print(f\"  These stores have fewer than {low_threshold:.0f} transactions\")\n",
    "    print(\"  Method: Percentile-based anomaly detection (bottom 5%)\")\n",
    "    print(\"  Rationale: These stores may be new locations, closing stores, or have data collection issues. Flagging allows for targeted analysis without removing potentially valid transactions.\")\n",
    "\n",
    "# 3.5 Check for inconsistencies between transactions and products\n",
    "trans_products = set(cleaned_trans['prod_id'].unique())\n",
    "catalog_products = set(cleaned_prods['prod_id'].unique())\n",
    "\n",
    "missing_product_info = trans_products - catalog_products\n",
    "if len(missing_product_info) > 0:\n",
    "    cleaned_trans['has_missing_product_info'] = cleaned_trans['prod_id'].isin(missing_product_info)\n",
    "    print(f\"• Found {len(missing_product_info):,} products in transactions that aren't in the product catalog\")\n",
    "    \n",
    "    # Count affected transactions\n",
    "    affected_trans = cleaned_trans[cleaned_trans['has_missing_product_info']]\n",
    "    print(f\"  This affects {len(affected_trans):,} transactions ({len(affected_trans)/len(cleaned_trans)*100:.2f}% of sample)\")\n",
    "    print(\"  Method: Created flag column to identify these transactions rather than removing them\")\n",
    "    print(\"  Rationale: These may be discontinued products, new products not yet in the catalog, or data entry errors. Flagging preserves transaction data while highlighting the inconsistency for further investigation.\")\n",
    "\n",
    "# Save cleaned data\n",
    "cleaned_trans.to_csv('transactions_cleaned.csv', index=False)\n",
    "cleaned_prods.to_csv('products_cleaned.csv', index=False)\n",
    "\n",
    "print(\"\\nData cleaning complete! Cleaned data saved to CSV files.\")\n",
    "print(\"Cleaning approach: Conservative data preservation with explicit flagging of anomalies\")\n",
    "print(\"Key principle: Maintained as much original data as possible while resolving inconsistencies and flagging unusual patterns for further analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80ac7134-aa47-419a-9d3a-28f138e9540a",
   "metadata": {},
   "source": [
    "# 7. Post-Cleaning Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71f1f708-d246-41d0-8b90-058ee3670c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n=== Creating Post-Cleaning Visualizations ===\")\n",
    "\n",
    "# 1. Distribution of sales amounts after cleaning\n",
    "plt.figure(figsize=(15, 7))\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.histplot(cleaned_trans['sales_amt'].clip(upper=cleaned_trans['sales_amt'].quantile(0.99)), \n",
    "             bins=50, kde=True, color='blue')\n",
    "plt.title('Sales Amount Distribution (99th Percentile)\\nAfter Cleaning', fontsize=14)\n",
    "plt.xlabel('Sales Amount ($)', fontsize=12)\n",
    "plt.ylabel('Frequency', fontsize=12)\n",
    "\n",
    "# Sales amount boxplot after cleaning\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.boxplot(y=cleaned_trans['sales_amt'].clip(upper=cleaned_trans['sales_amt'].quantile(0.99)), color='blue')\n",
    "plt.title('Sales Amount Boxplot (99th Percentile)\\nAfter Cleaning', fontsize=14)\n",
    "plt.ylabel('Sales Amount ($)', fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.savefig('visualizations/after_cleaning_sales_distribution.png')\n",
    "plt.show()\n",
    "\n",
    "# 2. Distribution of sales quantities after cleaning\n",
    "plt.figure(figsize=(15, 7))\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.histplot(cleaned_trans['sales_qty'].clip(upper=cleaned_trans['sales_qty'].quantile(0.99)), \n",
    "             bins=50, kde=True, color='green')\n",
    "plt.title('Sales Quantity Distribution (99th Percentile)\\nAfter Cleaning', fontsize=14)\n",
    "plt.xlabel('Sales Quantity', fontsize=12)\n",
    "plt.ylabel('Frequency', fontsize=12)\n",
    "\n",
    "# Sales quantity boxplot after cleaning\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.boxplot(y=cleaned_trans['sales_qty'].clip(upper=cleaned_trans['sales_qty'].quantile(0.99)), color='green')\n",
    "plt.title('Sales Quantity Boxplot (99th Percentile)\\nAfter Cleaning', fontsize=14)\n",
    "plt.ylabel('Sales Quantity', fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.savefig('visualizations/after_cleaning_quantity_distribution.png')\n",
    "plt.show()\n",
    "\n",
    "# 3. Missing values heatmap after cleaning\n",
    "plt.figure(figsize=(14, 6))\n",
    "cleaned_trans_missing = cleaned_trans.isnull()\n",
    "cleaned_prods_missing = cleaned_prods.isnull()\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.heatmap(cleaned_trans_missing.sample(min(1000, len(cleaned_trans_missing))), \n",
    "            cbar=False, cmap='viridis', yticklabels=False)\n",
    "plt.title('Missing Values in Transactions (Sample)\\nAfter Cleaning', fontsize=14)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.heatmap(cleaned_prods_missing.sample(min(1000, len(cleaned_prods_missing))), \n",
    "            cbar=False, cmap='viridis', yticklabels=False)\n",
    "plt.title('Missing Values in Products (Sample)\\nAfter Cleaning', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.savefig('visualizations/after_cleaning_missing_values.png')\n",
    "plt.show()\n",
    "\n",
    "# 4. Before vs After Comparison\n",
    "# Calculate current metrics\n",
    "current_missing_count = cleaned_trans.isnull().sum().sum() + cleaned_prods.isnull().sum().sum()\n",
    "current_neg_sales_count = len(cleaned_trans[cleaned_trans['sales_amt'] < 0])\n",
    "\n",
    "# Create comparison dataframe\n",
    "metrics = {\n",
    "    'Metric': ['Missing Values', 'Negative Sales'],\n",
    "    'Before': [\n",
    "        original_missing_count,\n",
    "        original_neg_sales_count,\n",
    "    ],\n",
    "    'After': [\n",
    "        current_missing_count,\n",
    "        current_neg_sales_count,\n",
    "    ]\n",
    "}\n",
    "metrics_df = pd.DataFrame(metrics)\n",
    "metrics_df['Improvement'] = metrics_df['Before'] - metrics_df['After']\n",
    "metrics_df['% Change'] = (metrics_df['Improvement'] / metrics_df['Before'] * 100).round(2)\n",
    "\n",
    "# Display comparison table\n",
    "print(\"\\nData Quality Improvements:\")\n",
    "print(metrics_df.to_string(index=False))\n",
    "\n",
    "# Visualize the improvements\n",
    "plt.figure(figsize=(12, 7))\n",
    "metrics_df.set_index('Metric', inplace=True)\n",
    "ax = metrics_df[['Before', 'After']].plot(kind='bar', color=['#ff7f0e', '#1f77b4'])\n",
    "for i, v in enumerate(metrics_df['Before']):\n",
    "    ax.text(i-0.15, v/2, str(v), color='white', fontweight='bold', ha='center')\n",
    "for i, v in enumerate(metrics_df['After']):\n",
    "    ax.text(i+0.15, v/2, str(v), color='white', fontweight='bold', ha='center')\n",
    "plt.title('Data Quality Improvements', fontsize=16)\n",
    "plt.ylabel('Count', fontsize=14)\n",
    "plt.legend(fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.savefig('visualizations/data_quality_improvements.png')\n",
    "plt.show()\n",
    "\n",
    "# 5. Flagged Anomalies Visualization\n",
    "anomaly_counts = pd.Series({\n",
    "    'Returns': cleaned_trans['is_return'].sum() if 'is_return' in cleaned_trans.columns else 0,\n",
    "    'Outlier Customers': cleaned_trans['is_outlier_customer'].sum() if 'is_outlier_customer' in cleaned_trans.columns else 0,\n",
    "    'Low Activity Stores': cleaned_trans['is_low_activity_store'].sum() if 'is_low_activity_store' in cleaned_trans.columns else 0,\n",
    "    'Missing Product Info': cleaned_trans['has_missing_product_info'].sum() if 'has_missing_product_info' in cleaned_trans.columns else 0\n",
    "})\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "ax = anomaly_counts.plot(kind='bar', color='purple')\n",
    "for i, v in enumerate(anomaly_counts):\n",
    "    ax.text(i, v + 0.1, f\"{v:,}\\n({v/len(cleaned_trans)*100:.1f}%)\", ha='center')\n",
    "plt.title('Flagged Anomalies in Cleaned Dataset', fontsize=16)\n",
    "plt.ylabel('Number of Transactions', fontsize=14)\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.savefig('visualizations/flagged_anomalies.png')\n",
    "plt.show()\n",
    "\n",
    "print(\"Post-cleaning visualizations created and displayed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b3f8f44-8e8d-431d-89bc-870d61c4cd46",
   "metadata": {},
   "source": [
    "# 8. Key Business Insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7afe33be-e7c8-4c17-8cde-9e72e5649aaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n=== Key Business Insights ===\")\n",
    "\n",
    "# 1. Top customers by revenue\n",
    "print(\"\\n--- Top Customers Analysis ---\")\n",
    "top_customers = customer_stats.sort_values('total_spent', ascending=False).head(10)\n",
    "print(\"Top 10 Customers by Total Spend:\")\n",
    "for i, (cust_id, row) in enumerate(top_customers.iterrows(), 1):\n",
    "    print(f\"{i}. Customer {cust_id}: ${row['total_spent']:,.2f} total spent, {row['num_transactions']:,} transactions, {row['unique_products']:,} unique products\")\n",
    "\n",
    "# Visualize top customers\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.bar(range(1, 11), top_customers['total_spent'], color='blue')\n",
    "plt.title('Top 10 Customers by Total Spend', fontsize=16)\n",
    "plt.xlabel('Customer Rank', fontsize=14)\n",
    "plt.ylabel('Total Spend ($)', fontsize=14)\n",
    "plt.xticks(range(1, 11))\n",
    "for i, v in enumerate(top_customers['total_spent']):\n",
    "    plt.text(i+1, v, f\"${v:,.0f}\", ha='center', va='bottom')\n",
    "plt.tight_layout()\n",
    "plt.savefig('visualizations/top_customers.png')\n",
    "plt.show()\n",
    "\n",
    "# 2. Top products and categories\n",
    "print(\"\\n--- Top Products and Categories Analysis ---\")\n",
    "\n",
    "# Join transactions with product info\n",
    "trans_with_products = cleaned_trans.merge(\n",
    "    cleaned_prods[['prod_id', 'prod_desc', 'prod_category', 'prod_subcategory']], \n",
    "    on='prod_id', \n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Top products by sales\n",
    "product_sales = trans_with_products.groupby(['prod_id', 'prod_desc'])['sales_amt'].sum().sort_values(ascending=False)\n",
    "top_products = product_sales.head(10)\n",
    "\n",
    "print(\"Top 10 Products by Sales Amount:\")\n",
    "for i, ((prod_id, prod_desc), sales) in enumerate(top_products.items(), 1):\n",
    "    print(f\"{i}. Product {prod_id} ({prod_desc}): ${sales:,.2f}\")\n",
    "\n",
    "# Visualize top products\n",
    "plt.figure(figsize=(14, 7))\n",
    "top_products_df = product_sales.head(10).reset_index()\n",
    "plt.barh(top_products_df['prod_desc'].str[:30], top_products_df['sales_amt'], color='green')\n",
    "plt.title('Top 10 Products by Sales Amount', fontsize=16)\n",
    "plt.xlabel('Sales Amount ($)', fontsize=14)\n",
    "plt.ylabel('Product Description', fontsize=14)\n",
    "plt.gca().invert_yaxis()  # Display highest at top\n",
    "for i, v in enumerate(top_products_df['sales_amt']):\n",
    "    plt.text(v, i, f\" ${v:,.0f}\", va='center')\n",
    "plt.tight_layout()\n",
    "plt.savefig('visualizations/top_products.png')\n",
    "plt.show()\n",
    "\n",
    "# Top categories by sales (if category information is available)\n",
    "if 'prod_category' in trans_with_products.columns:\n",
    "    category_sales = trans_with_products.groupby('prod_category')['sales_amt'].sum().sort_values(ascending=False)\n",
    "    top_categories = category_sales.head(10)\n",
    "    \n",
    "    print(\"\\nTop 10 Categories by Sales Amount:\")\n",
    "    for i, (category, sales) in enumerate(top_categories.items(), 1):\n",
    "        print(f\"{i}. {category}: ${sales:,.2f}\")\n",
    "    \n",
    "    # Visualize top categories\n",
    "    plt.figure(figsize=(14, 7))\n",
    "    plt.barh(top_categories.index, top_categories.values, color='purple')\n",
    "    plt.title('Top 10 Categories by Sales Amount', fontsize=16)\n",
    "    plt.xlabel('Sales Amount ($)', fontsize=14)\n",
    "    plt.ylabel('Product Category', fontsize=14)\n",
    "    plt.gca().invert_yaxis()  # Display highest at top\n",
    "    for i, v in enumerate(top_categories.values):\n",
    "        plt.text(v, i, f\" ${v:,.0f}\", va='center')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('visualizations/top_categories.png')\n",
    "    plt.show()\n",
    "\n",
    "# 3. Store analysis\n",
    "print(\"\\n--- Store Performance Analysis ---\")\n",
    "store_performance = cleaned_trans.groupby('store_id').agg({\n",
    "    'sales_amt': 'sum',\n",
    "    'trans_id': 'nunique',\n",
    "    'cust_id': 'nunique',\n",
    "    'prod_id': 'nunique'\n",
    "})\n",
    "store_performance.columns = ['total_sales', 'num_transactions', 'unique_customers', 'unique_products']\n",
    "store_performance['avg_transaction_value'] = store_performance['total_sales'] / store_performance['num_transactions']\n",
    "\n",
    "top_stores = store_performance.sort_values('total_sales', ascending=False).head(10)\n",
    "print(\"Top 10 Stores by Sales Amount:\")\n",
    "for i, (store_id, row) in enumerate(top_stores.iterrows(), 1):\n",
    "    print(f\"{i}. Store {store_id}: ${row['total_sales']:,.2f} total sales, {row['num_transactions']:,} transactions\")\n",
    "\n",
    "# Visualize top stores\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.bar(top_stores.index.astype(str), top_stores['total_sales'], color='orange')\n",
    "plt.title('Top 10 Stores by Sales Amount', fontsize=16)\n",
    "plt.xlabel('Store ID', fontsize=14)\n",
    "plt.ylabel('Total Sales ($)', fontsize=14)\n",
    "plt.xticks(rotation=45)\n",
    "for i, v in enumerate(top_stores['total_sales']):\n",
    "    plt.text(i, v, f\"${v:,.0f}\", ha='center', va='bottom')\n",
    "plt.tight_layout()\n",
    "plt.savefig('visualizations/top_stores.png')\n",
    "plt.show()\n",
    "\n",
    "# 4. Temporal analysis\n",
    "print(\"\\n--- Sales Trends Analysis ---\")\n",
    "# Group by month\n",
    "monthly_sales = cleaned_trans.groupby(cleaned_trans['trans_dt'].dt.to_period('M'))['sales_amt'].sum()\n",
    "monthly_sales.index = monthly_sales.index.astype(str)\n",
    "\n",
    "# Visualize monthly sales\n",
    "plt.figure(figsize=(14, 7))\n",
    "plt.plot(monthly_sales.index, monthly_sales.values, marker='o', linewidth=2, color='blue')\n",
    "plt.title('Monthly Sales Trends', fontsize=16)\n",
    "plt.xlabel('Month', fontsize=14)\n",
    "plt.ylabel('Total Sales ($)', fontsize=14)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.xticks(rotation=45)\n",
    "for i, v in enumerate(monthly_sales.values):\n",
    "    plt.text(i, v, f\"${v:,.0f}\", ha='center', va='bottom')\n",
    "plt.tight_layout()\n",
    "plt.savefig('visualizations/monthly_sales.png')\n",
    "plt.show()\n",
    "\n",
    "print(\"Key business insights analysis complete. Visualizations saved to 'visualizations' directory.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b08e89d2-f355-41fd-9ceb-3a65310f19f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the final cleaned datasets with appropriate names\n",
    "cleaned_trans.to_csv('acse_transactions_cleaned_final.csv', index=False)\n",
    "cleaned_prods.to_csv('acse_products_cleaned_final.csv', index=False)\n",
    "\n",
    "print(\"\\n=== Final Cleaned Datasets Saved ===\")\n",
    "print(f\"• Cleaned transactions dataset saved to 'acse_transactions_cleaned_final.csv' ({len(cleaned_trans):,} rows)\")\n",
    "print(f\"• Cleaned products dataset saved to 'acse_products_cleaned_final.csv' ({len(cleaned_prods):,} rows)\")\n",
    "print(\"• All detected anomalies are flagged rather than removed, preserving data integrity\")\n",
    "print(\"• The datasets are now ready for further analysis and building the recommender system\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61be28ec-e597-464d-ad00-5f95b9bc39e8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
